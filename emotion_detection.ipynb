{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "EMOTİON DETECTİON SEMESTER PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'fer2013.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(FILE_PATH)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = data['pixels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# görüntüleri ve duyguları yükler \n",
    "faces = []\n",
    "\n",
    "for p in pixels:\n",
    "    face = [int(pix) for pix in p.split(' ')]\n",
    "    face = np.asarray(face).reshape(width, height)\n",
    "    face = cv2.resize(face.astype('uint8'), image_size)\n",
    "    faces.append(face.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35887"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = np.asarray(faces)\n",
    "faces = np.expand_dims(faces, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = pd.get_dummies(data['emotion']).values\n",
    "emotions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizasyonu gerçekleştirir.0 ile 1 arasında değer almasını sağlar.2 ile çarpıp 0.5 ile çıkartarak da -1 ile 1 arasında değere dönüştürür. \n",
    "def preprocess(x, v2=True):  \n",
    "    x = x.astype('float32')\n",
    "    x = x/255.0\n",
    "    if v2:\n",
    "        x = (x - 0.5)*2.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = preprocess(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('örnek eğitilmiş resimleri göster')\n",
    "#for image in np.arange(0,10):\n",
    "#    cv2.namedWindow('örnek eğitilmiş resimleri göster', cv2.WINDOW_NORMAL)\n",
    "#    cv2.imshow('örnek eğitilmiş resimleri göster',faces[image])\n",
    "#    cv2.waitKey(500)\n",
    "#    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitme ve test etme için 4 değer döndürür.\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CNN model : Mini Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Activation, Convolution2D, Conv2D, Dropout, AveragePooling2D, BatchNormalization, GlobalAveragePooling2D, Flatten, Input, MaxPooling2D, SeparableConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters \n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "image_shape = (48, 48, 1)\n",
    "verbose = True \n",
    "num_class = 7\n",
    "patience = 50  # sonrasında eğitimin durdurulacağı gelişme olmayan dönemlerin sayısı\n",
    "base_path = 'models/'\n",
    "l2_regularization = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(featurewise_center=False, featurewise_std_normalization=False, rotation_range=10, \n",
    "                                    width_shift_range=0.1, height_shift_range=0.1, zoom_range=.1, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = l2(l2_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "image_input = Input(image_shape)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), kernel_regularizer=regularization, use_bias=False)(image_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# module 1\n",
    "# residual module \n",
    "residual = Conv2D(filters=16, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=16, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=16, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "# module 2\n",
    "# residual module \n",
    "residual = Conv2D(filters=32, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "# module 3\n",
    "# residual module \n",
    "residual = Conv2D(filters=64, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "# module 4\n",
    "# residual module \n",
    "residual = Conv2D(filters=128, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "x = Conv2D(filters=num_class, kernel_size=(3,3), padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "output = Activation('softmax', name='predictions')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]            \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 58,423\n",
      "Trainable params: 56,951\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# loss en son hatalarımızı hesaplamamızı sağlar.(Geriye doğru türev alır) Az ise iyi eğitilmiş olur.\n",
    "# Optimizer ile parametremizi buluruz.\n",
    "# Metrics modelimizin sonucunu değerlendirmeyi sağlar\n",
    "\n",
    "model = Model(image_input, output)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"Adam\", metrics = [\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks \n",
    "log_file_path = base_path + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=int(patience/4), verbose=verbose)\n",
    "\n",
    "trained_models_path = base_path + '_mini_xception'\n",
    "model_names = trained_models_path + '.{epoch:02d}_{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=model_names, monitor='val_loss', verbose=verbose, save_best_only=True)\n",
    "\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "898/897 [==============================] - 337s 375ms/step - loss: 1.7716 - acc: 0.3331 - val_loss: 1.8491 - val_acc: 0.3402\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.84906, saving model to models/_mini_xception.01_0.34.hdf5\n",
      "Epoch 2/100\n",
      "898/897 [==============================] - 326s 363ms/step - loss: 1.4861 - acc: 0.4453 - val_loss: 1.4928 - val_acc: 0.4462\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.84906 to 1.49281, saving model to models/_mini_xception.02_0.45.hdf5\n",
      "Epoch 3/100\n",
      "898/897 [==============================] - 313s 349ms/step - loss: 1.3772 - acc: 0.4853 - val_loss: 1.3385 - val_acc: 0.5024\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49281 to 1.33848, saving model to models/_mini_xception.03_0.50.hdf5\n",
      "Epoch 4/100\n",
      "898/897 [==============================] - 320s 356ms/step - loss: 1.3087 - acc: 0.5100 - val_loss: 1.3388 - val_acc: 0.5057\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.33848\n",
      "Epoch 5/100\n",
      "898/897 [==============================] - 326s 363ms/step - loss: 1.2699 - acc: 0.5238 - val_loss: 1.2491 - val_acc: 0.5375\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33848 to 1.24912, saving model to models/_mini_xception.05_0.54.hdf5\n",
      "Epoch 6/100\n",
      "898/897 [==============================] - 322s 358ms/step - loss: 1.2359 - acc: 0.5354 - val_loss: 1.3964 - val_acc: 0.4730\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24912\n",
      "Epoch 7/100\n",
      "898/897 [==============================] - 334s 372ms/step - loss: 1.2093 - acc: 0.5500 - val_loss: 1.2890 - val_acc: 0.5265\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24912\n",
      "Epoch 8/100\n",
      "898/897 [==============================] - 335s 373ms/step - loss: 1.1924 - acc: 0.5519 - val_loss: 1.4147 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.24912\n",
      "Epoch 9/100\n",
      "898/897 [==============================] - 322s 359ms/step - loss: 1.1749 - acc: 0.5599 - val_loss: 1.3399 - val_acc: 0.4929\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.24912\n",
      "Epoch 10/100\n",
      "898/897 [==============================] - 341s 380ms/step - loss: 1.1618 - acc: 0.5649 - val_loss: 1.2106 - val_acc: 0.5451\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.24912 to 1.21062, saving model to models/_mini_xception.10_0.55.hdf5\n",
      "Epoch 11/100\n",
      "898/897 [==============================] - 328s 365ms/step - loss: 1.1453 - acc: 0.5709 - val_loss: 1.1576 - val_acc: 0.5631\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.21062 to 1.15756, saving model to models/_mini_xception.11_0.56.hdf5\n",
      "Epoch 12/100\n",
      "898/897 [==============================] - 346s 385ms/step - loss: 1.1262 - acc: 0.5789 - val_loss: 1.1897 - val_acc: 0.5496\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.15756\n",
      "Epoch 13/100\n",
      "898/897 [==============================] - 353s 393ms/step - loss: 1.1163 - acc: 0.5786 - val_loss: 1.1418 - val_acc: 0.5801\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.15756 to 1.14177, saving model to models/_mini_xception.13_0.58.hdf5\n",
      "Epoch 14/100\n",
      "898/897 [==============================] - 356s 397ms/step - loss: 1.1055 - acc: 0.5884 - val_loss: 1.1180 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.14177 to 1.11799, saving model to models/_mini_xception.14_0.59.hdf5\n",
      "Epoch 15/100\n",
      "898/897 [==============================] - 358s 398ms/step - loss: 1.0952 - acc: 0.5896 - val_loss: 1.1509 - val_acc: 0.5793\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.11799\n",
      "Epoch 16/100\n",
      "898/897 [==============================] - 354s 394ms/step - loss: 1.0841 - acc: 0.5952 - val_loss: 1.1062 - val_acc: 0.5949\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.11799 to 1.10622, saving model to models/_mini_xception.16_0.59.hdf5\n",
      "Epoch 17/100\n",
      "898/897 [==============================] - 352s 392ms/step - loss: 1.0753 - acc: 0.5969 - val_loss: 1.1577 - val_acc: 0.5709\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.10622\n",
      "Epoch 18/100\n",
      "898/897 [==============================] - 353s 393ms/step - loss: 1.0709 - acc: 0.6009 - val_loss: 1.1515 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.10622\n",
      "Epoch 19/100\n",
      "898/897 [==============================] - 318s 354ms/step - loss: 1.0675 - acc: 0.6039 - val_loss: 1.1721 - val_acc: 0.5663\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.10622\n",
      "Epoch 20/100\n",
      "898/897 [==============================] - 323s 360ms/step - loss: 1.0544 - acc: 0.6081 - val_loss: 1.1010 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.10622 to 1.10103, saving model to models/_mini_xception.20_0.59.hdf5\n",
      "Epoch 21/100\n",
      "898/897 [==============================] - 322s 359ms/step - loss: 1.0517 - acc: 0.6063 - val_loss: 1.1279 - val_acc: 0.5782\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.10103\n",
      "Epoch 22/100\n",
      "898/897 [==============================] - 320s 356ms/step - loss: 1.0409 - acc: 0.6081 - val_loss: 1.2636 - val_acc: 0.5287\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.10103\n",
      "Epoch 23/100\n",
      "898/897 [==============================] - 323s 359ms/step - loss: 1.0402 - acc: 0.6106 - val_loss: 1.0891 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.10103 to 1.08912, saving model to models/_mini_xception.23_0.60.hdf5\n",
      "Epoch 24/100\n",
      "898/897 [==============================] - 314s 349ms/step - loss: 1.0348 - acc: 0.6158 - val_loss: 1.1813 - val_acc: 0.5602\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.08912\n",
      "Epoch 25/100\n",
      "898/897 [==============================] - 316s 352ms/step - loss: 1.0273 - acc: 0.6150 - val_loss: 1.0905 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.08912\n",
      "Epoch 26/100\n",
      "898/897 [==============================] - 316s 352ms/step - loss: 1.0240 - acc: 0.6185 - val_loss: 1.1921 - val_acc: 0.5756\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.08912\n",
      "Epoch 27/100\n",
      "898/897 [==============================] - 318s 354ms/step - loss: 1.0192 - acc: 0.6177 - val_loss: 1.1313 - val_acc: 0.5784\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.08912\n",
      "Epoch 28/100\n",
      "898/897 [==============================] - 334s 372ms/step - loss: 1.0180 - acc: 0.6220 - val_loss: 1.0992 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.08912\n",
      "Epoch 29/100\n",
      "898/897 [==============================] - 342s 381ms/step - loss: 1.0141 - acc: 0.6216 - val_loss: 1.0861 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.08912 to 1.08610, saving model to models/_mini_xception.29_0.60.hdf5\n",
      "Epoch 30/100\n",
      "898/897 [==============================] - 332s 370ms/step - loss: 1.0020 - acc: 0.6267 - val_loss: 1.0949 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.08610\n",
      "Epoch 31/100\n",
      "898/897 [==============================] - 351s 391ms/step - loss: 1.0006 - acc: 0.6276 - val_loss: 1.0531 - val_acc: 0.6116\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.08610 to 1.05309, saving model to models/_mini_xception.31_0.61.hdf5\n",
      "Epoch 32/100\n",
      "898/897 [==============================] - 357s 398ms/step - loss: 0.9977 - acc: 0.6283 - val_loss: 1.0874 - val_acc: 0.6013\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.05309\n",
      "Epoch 33/100\n",
      "898/897 [==============================] - 331s 369ms/step - loss: 0.9921 - acc: 0.6293 - val_loss: 1.0819 - val_acc: 0.5953\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.05309\n",
      "Epoch 34/100\n",
      "898/897 [==============================] - 324s 361ms/step - loss: 0.9877 - acc: 0.6298 - val_loss: 1.0960 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.05309\n",
      "Epoch 35/100\n",
      "898/897 [==============================] - 367s 409ms/step - loss: 0.9874 - acc: 0.6327 - val_loss: 1.0789 - val_acc: 0.6028\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.05309\n",
      "Epoch 36/100\n",
      "898/897 [==============================] - 364s 405ms/step - loss: 0.9784 - acc: 0.6349 - val_loss: 1.0622 - val_acc: 0.6144\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.05309\n",
      "Epoch 37/100\n",
      "898/897 [==============================] - 350s 390ms/step - loss: 0.9743 - acc: 0.6364 - val_loss: 1.1077 - val_acc: 0.6037\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.05309\n",
      "Epoch 38/100\n",
      "898/897 [==============================] - 384s 428ms/step - loss: 0.9833 - acc: 0.6345 - val_loss: 1.0767 - val_acc: 0.5972\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.05309\n",
      "Epoch 39/100\n",
      "898/897 [==============================] - 382s 425ms/step - loss: 0.9717 - acc: 0.6390 - val_loss: 1.0598 - val_acc: 0.6102\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.05309\n",
      "Epoch 40/100\n",
      "898/897 [==============================] - 355s 395ms/step - loss: 0.9685 - acc: 0.6431 - val_loss: 1.1263 - val_acc: 0.5947\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.05309\n",
      "Epoch 41/100\n",
      "898/897 [==============================] - 362s 403ms/step - loss: 0.9639 - acc: 0.6391 - val_loss: 1.0690 - val_acc: 0.6158\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.05309\n",
      "Epoch 42/100\n",
      "898/897 [==============================] - 375s 418ms/step - loss: 0.9646 - acc: 0.6400 - val_loss: 1.0820 - val_acc: 0.6177\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.05309\n",
      "Epoch 43/100\n",
      "898/897 [==============================] - 370s 412ms/step - loss: 0.9603 - acc: 0.6432 - val_loss: 1.0349 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.05309 to 1.03492, saving model to models/_mini_xception.43_0.61.hdf5\n",
      "Epoch 44/100\n",
      "898/897 [==============================] - 369s 411ms/step - loss: 0.9570 - acc: 0.6427 - val_loss: 1.0416 - val_acc: 0.6165\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.03492\n",
      "Epoch 45/100\n",
      "898/897 [==============================] - 356s 396ms/step - loss: 0.9558 - acc: 0.6444 - val_loss: 1.0902 - val_acc: 0.5963\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.03492\n",
      "Epoch 46/100\n",
      "898/897 [==============================] - 352s 392ms/step - loss: 0.9576 - acc: 0.6442 - val_loss: 1.0599 - val_acc: 0.6014\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.03492\n",
      "Epoch 47/100\n",
      "898/897 [==============================] - 353s 394ms/step - loss: 0.9490 - acc: 0.6488 - val_loss: 1.0380 - val_acc: 0.6162\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.03492\n",
      "Epoch 48/100\n",
      "898/897 [==============================] - 373s 415ms/step - loss: 0.9496 - acc: 0.6457 - val_loss: 1.0664 - val_acc: 0.6018\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.03492\n",
      "Epoch 49/100\n",
      "898/897 [==============================] - 360s 401ms/step - loss: 0.9464 - acc: 0.6445 - val_loss: 1.0448 - val_acc: 0.6193\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.03492\n",
      "Epoch 50/100\n",
      "898/897 [==============================] - 356s 397ms/step - loss: 0.9415 - acc: 0.6467 - val_loss: 1.0307 - val_acc: 0.6222\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.03492 to 1.03073, saving model to models/_mini_xception.50_0.62.hdf5\n",
      "Epoch 51/100\n",
      "898/897 [==============================] - 351s 391ms/step - loss: 0.9431 - acc: 0.6481 - val_loss: 1.0712 - val_acc: 0.6030\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.03073\n",
      "Epoch 52/100\n",
      "898/897 [==============================] - 341s 380ms/step - loss: 0.9402 - acc: 0.6501 - val_loss: 1.0916 - val_acc: 0.6078\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.03073\n",
      "Epoch 53/100\n",
      "898/897 [==============================] - 324s 361ms/step - loss: 0.9361 - acc: 0.6559 - val_loss: 1.1156 - val_acc: 0.5981\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.03073\n",
      "Epoch 54/100\n",
      "898/897 [==============================] - 330s 367ms/step - loss: 0.9372 - acc: 0.6505 - val_loss: 1.0896 - val_acc: 0.6039\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.03073\n",
      "Epoch 55/100\n",
      "898/897 [==============================] - 346s 386ms/step - loss: 0.9313 - acc: 0.6530 - val_loss: 1.0487 - val_acc: 0.6034\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.03073\n",
      "Epoch 56/100\n",
      "898/897 [==============================] - 341s 379ms/step - loss: 0.9341 - acc: 0.6523 - val_loss: 1.0517 - val_acc: 0.6126\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.03073\n",
      "Epoch 57/100\n",
      "898/897 [==============================] - 328s 365ms/step - loss: 0.9287 - acc: 0.6493 - val_loss: 1.0594 - val_acc: 0.6195\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.03073\n",
      "Epoch 58/100\n",
      "898/897 [==============================] - 340s 379ms/step - loss: 0.9355 - acc: 0.6516 - val_loss: 1.0369 - val_acc: 0.6209\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.03073\n",
      "Epoch 59/100\n",
      "898/897 [==============================] - 351s 391ms/step - loss: 0.9207 - acc: 0.6553 - val_loss: 1.0653 - val_acc: 0.6071\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.03073\n",
      "Epoch 60/100\n",
      "898/897 [==============================] - 337s 376ms/step - loss: 0.9203 - acc: 0.6591 - val_loss: 1.0331 - val_acc: 0.6278\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.03073\n",
      "Epoch 61/100\n",
      "898/897 [==============================] - 337s 375ms/step - loss: 0.9132 - acc: 0.6593 - val_loss: 1.0596 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.03073\n",
      "Epoch 62/100\n",
      "898/897 [==============================] - 355s 395ms/step - loss: 0.9177 - acc: 0.6569 - val_loss: 1.1000 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.03073\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 63/100\n",
      "898/897 [==============================] - 355s 395ms/step - loss: 0.8699 - acc: 0.6759 - val_loss: 1.0003 - val_acc: 0.6365\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.03073 to 1.00034, saving model to models/_mini_xception.63_0.64.hdf5\n",
      "Epoch 64/100\n",
      "898/897 [==============================] - 355s 396ms/step - loss: 0.8536 - acc: 0.6832 - val_loss: 0.9983 - val_acc: 0.6386\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.00034 to 0.99827, saving model to models/_mini_xception.64_0.64.hdf5\n",
      "Epoch 65/100\n",
      "898/897 [==============================] - 354s 395ms/step - loss: 0.8511 - acc: 0.6849 - val_loss: 0.9986 - val_acc: 0.6372\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.99827\n",
      "Epoch 66/100\n",
      "898/897 [==============================] - 355s 396ms/step - loss: 0.8464 - acc: 0.6866 - val_loss: 1.0011 - val_acc: 0.6369\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.99827\n",
      "Epoch 67/100\n",
      "898/897 [==============================] - 354s 394ms/step - loss: 0.8464 - acc: 0.6873 - val_loss: 0.9978 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.99827 to 0.99775, saving model to models/_mini_xception.67_0.64.hdf5\n",
      "Epoch 68/100\n",
      "898/897 [==============================] - 354s 394ms/step - loss: 0.8391 - acc: 0.6877 - val_loss: 0.9929 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.99775 to 0.99288, saving model to models/_mini_xception.68_0.64.hdf5\n",
      "Epoch 69/100\n",
      "898/897 [==============================] - 354s 394ms/step - loss: 0.8391 - acc: 0.6898 - val_loss: 0.9930 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.99288\n",
      "Epoch 70/100\n",
      "898/897 [==============================] - 355s 396ms/step - loss: 0.8381 - acc: 0.6907 - val_loss: 0.9985 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.99288\n",
      "Epoch 71/100\n",
      "898/897 [==============================] - 352s 392ms/step - loss: 0.8337 - acc: 0.6885 - val_loss: 0.9986 - val_acc: 0.6403\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.99288\n",
      "Epoch 72/100\n",
      "898/897 [==============================] - 347s 386ms/step - loss: 0.8314 - acc: 0.6926 - val_loss: 0.9950 - val_acc: 0.6428\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.99288\n",
      "Epoch 73/100\n",
      "898/897 [==============================] - 323s 360ms/step - loss: 0.8338 - acc: 0.6914 - val_loss: 0.9988 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.99288\n",
      "Epoch 74/100\n",
      "898/897 [==============================] - 319s 355ms/step - loss: 0.8307 - acc: 0.6928 - val_loss: 0.9934 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.99288\n",
      "Epoch 75/100\n",
      "898/897 [==============================] - 318s 354ms/step - loss: 0.8265 - acc: 0.6938 - val_loss: 0.9954 - val_acc: 0.6418\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.99288\n",
      "Epoch 76/100\n",
      "898/897 [==============================] - 8552s 10s/step - loss: 0.8249 - acc: 0.6950 - val_loss: 0.9956 - val_acc: 0.6421\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.99288\n",
      "Epoch 77/100\n",
      "898/897 [==============================] - 343s 382ms/step - loss: 0.8282 - acc: 0.6933 - val_loss: 0.9966 - val_acc: 0.6425\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.99288\n",
      "Epoch 78/100\n",
      "898/897 [==============================] - 334s 372ms/step - loss: 0.8239 - acc: 0.6961 - val_loss: 0.9980 - val_acc: 0.6397\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.99288\n",
      "Epoch 79/100\n",
      "898/897 [==============================] - 309s 344ms/step - loss: 0.8260 - acc: 0.6965 - val_loss: 0.9959 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.99288\n",
      "Epoch 80/100\n",
      "898/897 [==============================] - 310s 345ms/step - loss: 0.8269 - acc: 0.6896 - val_loss: 0.9962 - val_acc: 0.6410\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.99288\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 81/100\n",
      "898/897 [==============================] - 343s 382ms/step - loss: 0.8141 - acc: 0.6974 - val_loss: 0.9956 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.99288\n",
      "Epoch 82/100\n",
      "898/897 [==============================] - 335s 373ms/step - loss: 0.8182 - acc: 0.6953 - val_loss: 0.9958 - val_acc: 0.6431\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.99288\n",
      "Epoch 83/100\n",
      "898/897 [==============================] - 311s 346ms/step - loss: 0.8166 - acc: 0.6950 - val_loss: 0.9965 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.99288\n",
      "Epoch 84/100\n",
      "898/897 [==============================] - 311s 346ms/step - loss: 0.8176 - acc: 0.6969 - val_loss: 0.9966 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.99288\n",
      "Epoch 85/100\n",
      "898/897 [==============================] - 311s 346ms/step - loss: 0.8152 - acc: 0.6979 - val_loss: 0.9980 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.99288\n",
      "Epoch 86/100\n",
      "898/897 [==============================] - 885s 986ms/step - loss: 0.8158 - acc: 0.6978 - val_loss: 0.9961 - val_acc: 0.6431\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.99288\n",
      "Epoch 87/100\n",
      "898/897 [==============================] - 343s 382ms/step - loss: 0.8180 - acc: 0.6961 - val_loss: 0.9958 - val_acc: 0.6421\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.99288\n",
      "Epoch 88/100\n",
      "898/897 [==============================] - 329s 366ms/step - loss: 0.8151 - acc: 0.6991 - val_loss: 0.9954 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.99288\n",
      "Epoch 89/100\n",
      "898/897 [==============================] - 314s 350ms/step - loss: 0.8162 - acc: 0.6982 - val_loss: 0.9951 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.99288\n",
      "Epoch 90/100\n",
      "898/897 [==============================] - 344s 384ms/step - loss: 0.8121 - acc: 0.7004 - val_loss: 0.9950 - val_acc: 0.6436\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.99288\n",
      "Epoch 91/100\n",
      "898/897 [==============================] - 331s 368ms/step - loss: 0.8178 - acc: 0.6981 - val_loss: 0.9961 - val_acc: 0.6435\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.99288\n",
      "Epoch 92/100\n",
      "898/897 [==============================] - 320s 356ms/step - loss: 0.8172 - acc: 0.6979 - val_loss: 0.9946 - val_acc: 0.6460\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.99288\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 93/100\n",
      "898/897 [==============================] - 372s 415ms/step - loss: 0.8109 - acc: 0.7014 - val_loss: 0.9942 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.99288\n",
      "Epoch 94/100\n",
      "898/897 [==============================] - 382s 426ms/step - loss: 0.8140 - acc: 0.6974 - val_loss: 0.9955 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.99288\n",
      "Epoch 95/100\n",
      "898/897 [==============================] - 378s 421ms/step - loss: 0.8168 - acc: 0.6987 - val_loss: 0.9965 - val_acc: 0.6445\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.99288\n",
      "Epoch 96/100\n",
      "898/897 [==============================] - 376s 418ms/step - loss: 0.8130 - acc: 0.6968 - val_loss: 0.9953 - val_acc: 0.6445\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.99288\n",
      "Epoch 97/100\n",
      "898/897 [==============================] - 365s 406ms/step - loss: 0.8176 - acc: 0.6995 - val_loss: 0.9946 - val_acc: 0.6459\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.99288\n",
      "Epoch 98/100\n",
      "898/897 [==============================] - 358s 398ms/step - loss: 0.8112 - acc: 0.6984 - val_loss: 0.9959 - val_acc: 0.6453\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.99288\n",
      "Epoch 99/100\n",
      "898/897 [==============================] - 344s 383ms/step - loss: 0.8169 - acc: 0.6966 - val_loss: 0.9950 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.99288\n",
      "Epoch 100/100\n",
      "898/897 [==============================] - 347s 387ms/step - loss: 0.8125 - acc: 0.6981 - val_loss: 0.9955 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.99288\n",
      "28709/28709 [==============================] - 95s 3ms/step\n",
      "Eğitim doğruluğu: % 71.71967029571533\n",
      "7178/7178 [==============================] - 23s 3ms/step\n",
      "Test doğruluğu: % 64.48871493339539\n"
     ]
    }
   ],
   "source": [
    "# Eğitmenin gerçekleşmesi sağlanır. (epochs kaç kere eğiteleceğini gösterir,batch resimlerin kaç grup halde train edileceğini gösterir.)\n",
    "\n",
    "# model.fit(xtrain, ytrain, epochs=35, batch_size=64)\n",
    "\n",
    "model.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),\n",
    "                        steps_per_epoch=len(xtrain) / batch_size,\n",
    "                        epochs=epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=(xtest,ytest))\n",
    "\n",
    "# Değerlendirme işlemleri gerçekleştirilir.\n",
    "score_train = model.evaluate(xtrain, ytrain)\n",
    "print(\"Eğitim doğruluğu: %\",score_train[1]*100) # score[0] kaybı verir.\n",
    "    \n",
    "score_test = model.evaluate(xtest, ytest)\n",
    "print(\"Test doğruluğu: %\",score_test[1]*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bir görüntüdeki bir yüzün duygularını algılama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import imutils\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model_path = 'haarcascade_frontalface_default.xml'\n",
    "emotion_recognition_model_path = base_path + '_mini_xception.100_0.65.hdf5'\n",
    "image_path = 'duygu-1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection = cv2.CascadeClassifier(detection_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = load_model(emotion_recognition_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['angry', 'disgust', 'scared', 'happy', 'sad', 'surprised', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_frame = cv2.imread(image_path)\n",
    "gray_frame = cv2.imread(image_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Input test image', color_frame)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected :  18\n"
     ]
    }
   ],
   "source": [
    "detected_faces = face_detection.detectMultiScale(color_frame, scaleFactor=1.1, minNeighbors=5, \n",
    "                                        minSize=(30,30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "print('Number of faces detected : ', len(detected_faces))\n",
    "\n",
    "if len(detected_faces)>0:\n",
    "\n",
    " # Birden fazla yüz olduğu durumda   \n",
    "    detected_faces = sorted(detected_faces, reverse=True, key=lambda x: (x[2]-x[0])*(x[3]-x[1]))[3] \n",
    "    (fx, fy, fw, fh) = detected_faces\n",
    "    \n",
    "    im = gray_frame[fy:fy+fh, fx:fx+fw]\n",
    "    im = cv2.resize(im, (48,48))   \n",
    "    im = im.astype(\"float\")/255.0\n",
    "    im = img_to_array(im)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    \n",
    "    preds = emotion_classifier.predict(im)[0]\n",
    "    emotion_probability = np.max(preds)\n",
    "    label = emotions[preds.argmax()]\n",
    "    \n",
    "    cv2.putText(color_frame, label, (fx, fy-10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    cv2.rectangle(color_frame, (fx, fy), (fx + fw, fy + fh),(0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow('Input test image', color_frame)\n",
    "cv2.imwrite('output_'+image_path.split('/')[-1], color_frame)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bir videodaki yüzlerin duygularını algılama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-ce03a6a1c036>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mgray_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdetected_faces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_frame\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCASCADE_SCALE_IMAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv2.namedWindow('emotion_recognition')\n",
    "camera = cv2.VideoCapture(0)  ## bilgisayar kamerası kullanılır. \n",
    "## camera = cv2.VideoCapture('various_emotions.mp4')  # video dosyasından okur.\n",
    "\n",
    "sz = (int(camera.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mpeg')\n",
    "\n",
    "out = cv2.VideoWriter()\n",
    "out.open('output_various_emotions.mp4',fourcc, 15, sz, True) # videoya yazmaya başlar\n",
    "\n",
    "\n",
    "# while True: # kameradan video okunurken while kullanılır\n",
    "while(camera.read()[0]):  # dosyadan okurken de while kullanılır.\n",
    "    color_frame = camera.read()[1]\n",
    "    color_frame = imutils.resize(color_frame,width=min(720, color_frame.shape[1]))\n",
    "    \n",
    "    \n",
    "    gray_frame = cv2.cvtColor(color_frame, cv2.COLOR_BGR2GRAY)\n",
    "    detected_faces = face_detection.detectMultiScale(gray_frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    \n",
    "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "    frameClone = color_frame.copy()    \n",
    "\n",
    "    \n",
    "    if len(detected_faces)>0:\n",
    "\n",
    "        detected_faces = sorted(detected_faces, reverse=True, key=lambda x: (x[2]-x[0])*(x[3]-x[1]))[0] # birden fazla yüz varsa\n",
    "        (fx, fy, fw, fh) = detected_faces\n",
    "\n",
    "        im = gray_frame[fy:fy+fh, fx:fx+fw]\n",
    "        im = cv2.resize(im, (48,48))  # model 48*48 piksel görüntü üzerinde eğitilmiştir. \n",
    "        im = im.astype(\"float\")/255.0\n",
    "        im = img_to_array(im)\n",
    "        im = np.expand_dims(im, axis=0)\n",
    "\n",
    "        preds = emotion_classifier.predict(im)[0]\n",
    "        emotion_probability = np.max(preds)\n",
    "        label = emotions[preds.argmax()]\n",
    "\n",
    "        cv2.putText(color_frame, label, (fx, fy-10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "        cv2.rectangle(color_frame, (fx, fy), (fx + fw, fy + fh),(0, 0, 255), 2)\n",
    "\n",
    "    \n",
    "    for (i, (emotion, prob)) in enumerate(zip(emotions, preds)):\n",
    "        # construct the label text\n",
    "        text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "        w = int(prob * 300)\n",
    "        \n",
    "        cv2.rectangle(canvas, (7, (i * 35) + 5), (w, (i * 35) + 35), (0, 50, 100), -1)\n",
    "        cv2.putText(canvas, text, (10, (i * 35) + 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 1)\n",
    "        cv2.putText(frameClone, label, (fx, fy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (100, 150, 100), 2)\n",
    "        cv2.rectangle(frameClone, (fx, fy), (fx + fw, fy + fh), (100, 100, 100), 2)\n",
    "    \n",
    "    out.write(frameClone)\n",
    "    out.write(canvas)\n",
    "    \n",
    "    cv2.imshow('emotion_recognition', frameClone)\n",
    "    cv2.imshow(\"Probabilities\", canvas)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "camera.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
